---
title: "R package to tune parameters using Bayesian Optimization"
subtitle: "{MlBayesOpt}"
author: "@y__mattu"
date: "Global Tokyo.R #2 <br> April 1st, 2017"
output:
  revealjs::revealjs_presentation:
    transition: convex
    css: for_revealjs.css
    theme: sky
    highlight: kate
    center: true
    self_contained: false
    reveal_plugins: ["chalkboard"]
    reveal_options:
      slideNumber: true
      chalkboard:
        theme: whiteboard
---

```{r eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, comment=""}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      comment = "",
                      fig.height = 7,
                      fig.width = 7,
                      out.height = 400,
                      out.width = 400)
```

# Introduction

## Profile

<div class="column1">
- MATSUMURA Yuya <br> (松村優哉, @y__mattu)
- Graduate student of Keio University
- Studying: Econometrics, Bayesian Statistics, Causal Inference
- Languages: R, SAS, Python
</div>

<div class="column2">
![icon](./slide_img/twitter_icon.jpg)

## Agenda
- Summary of this package
- Motivation
- Usage
- Future works

# Summary of this package
## About this package
- MlBayesOpt(https://github.com/ymattu/MlBayesOpt)
- Tis package make it easier to write a script to execute parameter tuning using  bayesian optimization.
- SVM(RBF kernel)、Random Forest、XGboost
- Based on SVM(e1071), RF(ranger), XGboost(xgboost), Bayesian Optimization(rBayesianoOtimization)
- Using Hold-out validation

# Motivation to make this package
## How to execute Bayesian Optimization so far
### ex. XGboost
```{r eval = FALSE}
library(xgboost)
library(Matrix)
library(rBayesianOptimization)

odd.n <- 2*(1:75)-1
iris_train <- iris[odd.n, ] # odd numbered rows for training data
iris_test <- iris[-odd.n, ] # even numbered rows for test data
```

## How to execute Bayesian Optimization so far
```{r eval=FALSE}
# resahpe in order to deal with {xgboost} package
train.mx <- sparse.model.matrix(Species ~., iris_train)
test.mx <- sparse.model.matrix(Species ~ ., iris_test)
dtrain <- xgb.DMatrix(train.mx, label = as.integer(iris_train$Species) - 1)
dtest <- xgb.DMatrix(test.mx, label = as.integer(iris_test$Species) - 1)

# make a function to maximize
xgb_holdout <- function(ex, mx, nr){
    model <- xgb.train(params=list(objective = "multi:softmax", num_class = 10, eval_metric = "mlogloss",
                                   eta = ex, max_depth = mx),
                                   data = dtrain, nrounds = nr)
    t.pred <- predict(model, newdata = dtest)
    Pred <- sum(diag(table(test$label, t.pred)))/nrow(test)
    list(Score = Pred, Pred = Pred)
}
```

## How to execute Bayesian Optimization so far
```{r eval=FALSE}
# Bayesian Optimization
opt_xgb <- BayesianOptimization(xgb_holdout,
                                bounds=list(ex = c(2L,5L), mx = c(4L,5L), nr = c(70L,160L)),
                                init_points = 20, n_iter = 1, acq = 'ei', kappa = 2.576,
                                eps = 0.0, verbose = TRUE)

```
A long and bothering code !

# {MlBayesOpt}

## Installation and Reading
### Installation
```{r eval=FALSE}
devtools::install_github("ymattu/MlBayesOpt")
```

### Reading
```{r eval=FALSE}
library(MlBayesOpt)
```

# Usage
## SVM
```{r eval=FALSE}
set.seed(123)

res <- svm_opt(
  train_data = iris_train,
  train_label = iris_train$Species,
  test_data = iris_test,
  test_label = iris_test$Species,
  acq = "ucb"
  )
```
